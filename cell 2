"""
CELL 2: Define the ultra-fast inference function
Requires CELL 1 to be run first!

NOTE: onnxruntime is NOT imported locally - it only runs inside Modal containers.
All inference code runs remotely on Modal's GPU infrastructure.

UPDATED: Modal 1.0 API with immediate shutdown for cost savings
"""

import modal
import time
import numpy as np
import json
import concurrent.futures
from typing import List

# Import from CELL 1
try:
    from __main__ import disinfo_image, CONFIG, MODEL_DIR, hf_secret
except ImportError:
    # If running in notebook, these will be defined already
    pass

app = modal.App("disinfo-ultra-fast")

# =============================================================================
# HELPER FUNCTIONS
# =============================================================================

def softmax(x):
    """Fast softmax implementation"""
    exp_x = np.exp(x - np.max(x, axis=-1, keepdims=True))
    return exp_x / np.sum(exp_x, axis=-1, keepdims=True)

def create_ort_session(path, use_gpu: bool = True):
    """Create an optimized ONNX Runtime session for CPU or GPU
    
    NOTE: This function only runs inside Modal containers where onnxruntime is installed.
    """
    # Import here - only available inside Modal container
    import onnxruntime as ort
    
    opts = ort.SessionOptions()
    opts.graph_optimization_level = ort.GraphOptimizationLevel.ORT_ENABLE_ALL
    opts.execution_mode = ort.ExecutionMode.ORT_PARALLEL
    opts.intra_op_num_threads = 4
    opts.inter_op_num_threads = 4
    opts.enable_mem_pattern = True
    opts.enable_cpu_mem_arena = True
    
    if use_gpu:
        providers = [
            ('CUDAExecutionProvider', {
                'device_id': 0,
                'arena_extend_strategy': 'kSameAsRequested',
                'gpu_mem_limit': 4 * 1024 * 1024 * 1024,  # 4GB
                'cudnn_conv_algo_search': 'HEURISTIC',
                'do_copy_in_default_stream': True,
                'cudnn_conv_use_max_workspace': '1',
                'enable_cuda_graph': '1',  # Additional speedup
            }),
            ('CPUExecutionProvider', {}),
        ]
    else:
        providers = [('CPUExecutionProvider', {})]
    
    return ort.InferenceSession(path, opts, providers=providers)

def efficient_chunking(texts: List[str], tokenizer, max_length: int, overlap: int):
    """Ultra-optimized chunking with better memory management"""
    encoded = tokenizer(
        texts,
        padding='max_length',
        truncation=True,
        max_length=max_length,
        return_tensors="np",
        stride=overlap,
        return_overflowing_tokens=True,
        return_attention_mask=True,
        return_token_type_ids=False,
    )
    return encoded

def run_base_model(texts: List[str], session, tokenizer, model_name: str):
    """Optimized inference with vectorized chunk aggregation"""
    
    # Early return for single short text (avoid chunking overhead)
    if len(texts) == 1:
        encoded = tokenizer(
            texts[0],
            padding=True,
            truncation=True,
            max_length=CONFIG["MAX_LENGTH"],
            return_tensors="np",
        )
        inputs = {
            'input_ids': encoded['input_ids'].astype(np.int64),
            'attention_mask': encoded['attention_mask'].astype(np.int64)
        }
        outputs = session.run(None, inputs)
        logits = outputs[0] if len(outputs[0].shape) == 2 else outputs[1]
        return softmax(logits)[:, 1]
    
    # Standard chunking for batches
    encoded = efficient_chunking(texts, tokenizer, CONFIG["MAX_LENGTH"], CONFIG["CHUNK_OVERLAP"])
    
    inputs = {
        'input_ids': encoded['input_ids'].astype(np.int64),
        'attention_mask': encoded['attention_mask'].astype(np.int64)
    }
    outputs = session.run(None, inputs)
    
    all_chunk_logits = outputs[0]
    if len(all_chunk_logits.shape) == 3 and len(outputs) > 1:
        all_chunk_logits = outputs[1]
    
    num_texts = len(texts)
    num_labels = all_chunk_logits.shape[1]
    
    if "overflow_to_sample_mapping" in encoded:
        sample_mapping = encoded['overflow_to_sample_mapping']
        
        counts = np.bincount(sample_mapping, minlength=num_texts).astype(np.float32)
        sums = np.zeros((num_texts, num_labels), dtype=np.float32)
        np.add.at(sums, sample_mapping, all_chunk_logits.astype(np.float32))

        mean_logits = np.zeros_like(sums)
        valid_mask = counts > 0
        mean_logits[valid_mask] = sums[valid_mask] / counts[valid_mask, np.newaxis]
        
        final_probs_all = softmax(mean_logits)
        final_probs = final_probs_all[:, 1]
        final_probs[~valid_mask] = 0.0
    else:
        final_probs = softmax(all_chunk_logits)[:, 1]
    
    return final_probs

# =============================================================================
# GPU-ACCELERATED CLASS WITH MEMORY SNAPSHOTS (Modal 1.0 API)
# =============================================================================

@app.cls(
    image=disinfo_image,
    gpu="t4",
    secrets=[hf_secret],
    enable_memory_snapshot=True,
    scaledown_window=2,  # MINIMUM: Shutdown after 2 seconds idle (cost savings!)
    min_containers=0,    # No warm containers (maximum cost savings)
    max_containers=10,   # Scale up to 10 if needed
    experimental_options={
        "enable_gpu_snapshot": True,
    },
)
@modal.concurrent(max_inputs=1)  # NEW Modal 1.0 API: Process 1 request at a time
class DisinfoDetector:
    
    @modal.enter(snap=True)
    def load_models(self):
        """Optimized: Load all models with proper settings"""
        import os
        from transformers import AutoTokenizer
        
        start = time.perf_counter()
        token = os.environ["HF_TOKEN"]
        
        print("Loading tokenizers and models...")
        
        # Load tokenizers with optimized settings
        self.deberta_tokenizer = AutoTokenizer.from_pretrained(
            CONFIG["DEBERTA_TOKENIZER"],
            use_fast=True,
            token=token,
            cache_dir=MODEL_DIR,
            local_files_only=True,
            trust_remote_code=False,
            model_max_length=CONFIG["MAX_LENGTH"],
        )
        
        self.distilroberta_tokenizer = AutoTokenizer.from_pretrained(
            CONFIG["DISTILROBERTA_TOKENIZER"],
            use_fast=True,
            token=token,
            cache_dir=MODEL_DIR,
            local_files_only=True,
            trust_remote_code=False,
            model_max_length=CONFIG["MAX_LENGTH"],
        )
        
        # Load ONNX sessions with optimized settings
        self.deberta_session = create_ort_session(
            os.path.join(MODEL_DIR, CONFIG["DEBERTA_ONNX"]), 
            use_gpu=True
        )
        self.distilroberta_session = create_ort_session(
            os.path.join(MODEL_DIR, CONFIG["DISTILROBERTA_ONNX"]), 
            use_gpu=True
        )
        self.quantum_session = create_ort_session(
            os.path.join(MODEL_DIR, CONFIG["QUANTUM_ONNX"]), 
            use_gpu=False
        )
        
        # OPTIMIZATION: Warm-up runs to compile CUDA kernels
        print("Warming up GPU kernels...")
        dummy_text = ["This is a test article to warm up the models."]
        
        # Warm up DeBERTa
        dummy_inputs = self.deberta_tokenizer(
            dummy_text,
            padding=True,
            truncation=True,
            max_length=CONFIG["MAX_LENGTH"],
            return_tensors="np",
        )
        self.deberta_session.run(None, {
            'input_ids': dummy_inputs['input_ids'].astype(np.int64),
            'attention_mask': dummy_inputs['attention_mask'].astype(np.int64)
        })
        
        # Warm up DistilRoBERTa
        dummy_inputs = self.distilroberta_tokenizer(
            dummy_text,
            padding=True,
            truncation=True,
            max_length=CONFIG["MAX_LENGTH"],
            return_tensors="np",
        )
        self.distilroberta_session.run(None, {
            'input_ids': dummy_inputs['input_ids'].astype(np.int64),
            'attention_mask': dummy_inputs['attention_mask'].astype(np.int64)
        })
        
        elapsed = (time.perf_counter() - start) * 1000
        print(f"✅ Models loaded and warmed up in {elapsed:.2f} ms (cached in snapshot)")
    
    @modal.method()
    def detect(self, texts: List[str]) -> dict:
        """Optimized: Run inference with conditional parallelism"""
        if isinstance(texts, str):
            texts = [texts]
        
        overall_start = time.perf_counter()
        
        # OPTIMIZATION: For single text, run sequentially (avoid thread overhead)
        if len(texts) == 1:
            deberta_probs = run_base_model(
                texts, self.deberta_session, self.deberta_tokenizer, "DeBERTa"
            )
            distilroberta_probs = run_base_model(
                texts, self.distilroberta_session, self.distilroberta_tokenizer, "DistilRoBERTa"
            )
        else:
            # For batches, use parallel execution
            with concurrent.futures.ThreadPoolExecutor(max_workers=2) as executor:
                deberta_future = executor.submit(
                    run_base_model, texts, self.deberta_session, self.deberta_tokenizer, "DeBERTa"
                )
                distilroberta_future = executor.submit(
                    run_base_model, texts, self.distilroberta_session, self.distilroberta_tokenizer, "DistilRoBERTa"
                )
                deberta_probs = deberta_future.result()
                distilroberta_probs = distilroberta_future.result()
        
        # Stage 2: CPU stacking
        stacked = np.column_stack([deberta_probs, distilroberta_probs]).astype(np.float32)
        quantum_outputs = self.quantum_session.run(None, {'model_predictions': stacked})
        
        # Stage 3: Final predictions
        final_logits = quantum_outputs[0]
        final_probs = softmax(final_logits)
        predictions = np.argmax(final_logits, axis=1)
        
        labels = ["REAL" if pred == 0 else "FAKE" for pred in predictions]
        legitimate_probs = final_probs[:, 0]
        fake_probs = final_probs[:, 1]
        
        total_ms = (time.perf_counter() - overall_start) * 1000
        
        return {
            "predictions": predictions.tolist(),
            "fake_probabilities": fake_probs.tolist(),
            "real_probabilities": legitimate_probs.tolist(),
            "labels": labels,
            "timings": {
                "total_ms": total_ms,
                "per_text_ms": total_ms / len(texts),
                "throughput": len(texts) / (total_ms / 1000)
            }
        }

print("✅ Inference function defined with Modal 1.0 API!")
print("✅ Cost-optimized: Containers shut down after 2 seconds idle")
print("✅ GPU snapshots enabled for <1s cold starts")
print("Run CELL 3 next to test with your articles.")
