"""
ULTRA-OPTIMIZED ONNX Inference with GPU Memory Snapshots
Target: Sub-1s cold start, maximum throughput

CELL 1: Run this first to define the image (reusable across models)
"""

import modal
import time
import os
from typing import List

# =============================================================================
# CONFIGURATION
# =============================================================================

CONFIG = {
    "MODEL_REPO": "Feargal/cpu-onnx-model",
    "DEBERTA_ONNX": "model.onnx",
    "DISTILROBERTA_ONNX": "distil_roberta.onnx",
    "QUANTUM_ONNX": "quantum_oof_stacking.onnx",
    "DEBERTA_TOKENIZER": "Feargal/de_berta_model",
    "DISTILROBERTA_TOKENIZER": "Feargal/distil-roberta",
    "MAX_LENGTH": 512,
    "CHUNK_OVERLAP": 50,
}

MODEL_DIR = "/model_cache"
hf_secret = modal.Secret.from_name("huggingface-secret")

# =============================================================================
# IMAGE DEFINITION (Reusable!)
# =============================================================================

def download_models():
    """Download models during build - runs once per deployment"""
    from huggingface_hub import hf_hub_download
    from transformers import AutoTokenizer
    
    os.makedirs(MODEL_DIR, exist_ok=True)
    token = os.environ["HF_TOKEN"]
    
    # Download ONNX models
    for filename in [CONFIG["DEBERTA_ONNX"], CONFIG["DISTILROBERTA_ONNX"], CONFIG["QUANTUM_ONNX"]]:
        print(f"Downloading {filename}...")
        hf_hub_download(
            repo_id=CONFIG["MODEL_REPO"],
            filename=filename,
            token=token,
            cache_dir=MODEL_DIR,
            force_filename=filename,
        )
    
    # Download tokenizers
    for model_name in [CONFIG["DEBERTA_TOKENIZER"], CONFIG["DISTILROBERTA_TOKENIZER"]]:
        print(f"Downloading tokenizer for {model_name}...")
        AutoTokenizer.from_pretrained(
            model_name,
            use_fast=True,
            token=token,
            cache_dir=MODEL_DIR,
        )
    print("✅ All models downloaded")

def optimize_onnx_models():
    """Pre-compile and optimize ONNX models during image build"""
    import onnxruntime as ort
    
    for filename in [CONFIG["DEBERTA_ONNX"], CONFIG["DISTILROBERTA_ONNX"], CONFIG["QUANTUM_ONNX"]]:
        path = os.path.join(MODEL_DIR, filename)
        print(f"Pre-compiling {filename}...")
        
        opts = ort.SessionOptions()
        opts.graph_optimization_level = ort.GraphOptimizationLevel.ORT_ENABLE_ALL
        
        # Trigger compilation (will be cached)
        try:
            session = ort.InferenceSession(path, opts)
            del session
        except Exception as e:
            print(f"Note: {filename} will be compiled at runtime ({e})")
    
    print("✅ Models pre-compiled")

# Minimal, fast-booting image with CUDA 12.6 and latest packages
disinfo_image = (
    modal.Image.from_registry(
        "nvidia/cuda:12.6.0-cudnn9-runtime-ubuntu22.04",
        add_python="3.12",
    )
    .pip_install(
        # Latest stable versions (November 2025)
        "onnxruntime-gpu==1.23.2",      # Latest with CUDA 12 support
        "transformers==4.57.1",          # Latest
        "tokenizers==0.22.1",            # Latest
        "numpy==2.0.2",                  # Latest
        "huggingface-hub==0.27.0",       # Latest
        "sentencepiece==0.2.0",          # Stable
    )
    .run_function(download_models, secrets=[hf_secret], timeout=600)
    .run_function(optimize_onnx_models, secrets=[hf_secret], timeout=300)
)

print("✅ Image defined with CUDA 12.6 and latest packages!")
print("✅ This image can be reused across multiple models!")
print("Run CELL 2 next to define the inference function.")
